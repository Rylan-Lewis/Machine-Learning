{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>LEWIS_RYLAN_NATHAN_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: RYLAN NATHAN LEWIS\n",
    "<br>\n",
    "Github Username: Rylannat\n",
    "<br>\n",
    "USC ID: 8358130873 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(\"../data/AReM/AReM\")\n",
    "activities = [\"bending1\", \"bending2\", \"cycling\", \"lying\", \"sitting\", \"standing\", \"walking\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, on cleaning datasets and moving to execute further tasks ahead, I came across a 'NaN' anomaly for dataset 4 of the bending2 class in c(ii). On further moving ahead to c(iii), I got all values as NaN due to the bending2 NaN values. Now either deleting the entire dataset4 csv or doing something to fix it were the only available options. But since dataset4 also contains relevant information related to the bending2 class, completely getting rid of it didnt seem like a good idea as it would result in relevant information loss about the bending2 class and a problem of class imbalance w.r.t bending 2 class. Now since it was just one dataset instance that behaved like this, I decided to manually fix this error using excel itself. I opened the .csv file saved on my desktop, and noticed that in this file, all the values have been stored under the column:time column itself. And so i understood that due to the data cleaning command \"df.dropna(axis=1)\", it woudlve dropped all the time series feature columns as they were all empty, resulting in no field having the appropriate numeric value to extract time series features for, thus outputting NaN. So i selected the entire column, and did the following: Data -> Text to columns -> Delimiter -> Selected options Tab and Space. This ensured that all the values are now properly spread out and assigned to their own respective columns. After this I restarted the execution from the start again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data ={}\n",
    "test_data ={}\n",
    "\n",
    "for activity in activities:\n",
    "    activity_path = os.path.join(base_dir, activity)\n",
    "    if not os.path.isdir(activity_path):\n",
    "        continue\n",
    "\n",
    "    files = sorted([f for f in os.listdir(activity_path) if f .endswith(\".csv\")])\n",
    "\n",
    "    if activity in [\"bending1\", \"bending2\"]:\n",
    "        test_files = ['dataset1.csv', 'dataset2.csv']\n",
    "    else:\n",
    "        test_files = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n",
    "\n",
    "    train_data[activity] = []\n",
    "    test_data[activity] = []\n",
    "\n",
    "    for f in files:\n",
    "        file_path = os.path.join(activity_path, f)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, skiprows=4, on_bad_lines = 'skip')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}\")\n",
    "            continue\n",
    "\n",
    "        #Data cleaning part: First drops all rows having all fields NaN. Then checks all columns having all fields NaN and drops if exists. \n",
    "        #Then it converts all the fields to numeric, and in the case of another datatype (string for example) existing in any of the fields, it converts it to NaN\n",
    "        #Then it uses forward fill and backward fill methods to fill the newly obtained NaN values from conversion step just before.\n",
    "        df.dropna(how = \"all\", inplace = True)\n",
    "        df.dropna(axis=1,how = 'all', inplace = True)\n",
    "        df = df.apply(pd.to_numeric, errors = 'coerce')\n",
    "        df.ffill(inplace = True)\n",
    "        df.bfill(inplace = True)\n",
    "\n",
    "        df[\"activity\"] = activity\n",
    "\n",
    "        if f in test_files:\n",
    "            test_data[activity].append(df)\n",
    "        else:\n",
    "            train_data[activity].append(df)\n",
    "\n",
    "train_df = pd.concat([pd.concat(v,ignore_index = True) for v in train_data.values()], ignore_index = True)\n",
    "test_df = pd.concat([pd.concat(v,ignore_index = True) for v in test_data.values()], ignore_index = True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 main domains of time series features that are used for time series classification:\n",
    "\n",
    "a) Statistical features:\n",
    "    \n",
    "    1) Mean: The average value of the time series data.\n",
    "    2) Median: The midpoint value of the time series data.\n",
    "    3) Minimum: The smallest observed value in the entire time series data.\n",
    "    4) Maximum: The largest observed value in the entire time series data.\n",
    "    5) Range: The difference between the max and min which gives the distribution of the values of the time series data.\n",
    "    6) Standard Deviation: Measures the variability around the mean of the time series data.\n",
    "    7) Variance: Square of the standard deviation.\n",
    "    8) Quartiles: Divides the data into 4 equal size groups(0-25,25-50,50-75,75-100).\n",
    "    9) Interquartile Range: Shows the spread between the 25th and 75th percentile of the data.\n",
    "    10) Skewness: Shows the asymmetry of the distribution, showing whether the data is concentrated on one side of the mean.\n",
    "    11) Kurtosis: Shows the measure of peakedness OR tail heaviness of the distribution, which describes the extent to which its tails contain extreme values (outliers) compared to a normal distribution.\n",
    "    \n",
    "b) Time-domain features:\n",
    "\n",
    "    1) Autocorrelation: A correlation of the time series with a lagged version of itself to identify patterns and trends between the features in the data.\n",
    "    2) Partial Autocorrelation: Checks autocorrelation but without the effects of immediate lags on the relation between the features.\n",
    "    4) Slope: Rate of change in data over time. Helps explaining the trend of data wether is upward or downward based on its value. Usually computed using a SLR over the given time window.\n",
    "    5) No. of peaks OR valleys: Gives a count of how many times the series attains local maximum or minimum.\n",
    "    6) Longest consecutive Run: Gives a snapshot of the longest sequence of identical values OR values that fall above or below a certain threshold.\n",
    "    7) Zero Crossing Rate: No. of time the time signal crosses zero and oscillates between positive and negative signal values.\n",
    "    8) Energy: The sum of the squares of the values of the time series\n",
    "    9) Entropy: Gives a measure of the unpredictability or randomness of the series. Higher entropy means more randomness. Ex: Shannon, Approximate, Sample entropy, etc.\n",
    "    10) Root Mean Square(RMS): It is calculated using the square root of the mean of the squares of the values (Energy). So in short, it is a measure of signal magnitude and energy.\n",
    "    11) Mean Absolute Deviation: It is the average distance from the mean of the time series data.\n",
    "\n",
    "\n",
    "c) Frequency-domain features: These features are derived after transforming the time series into the frequency domain\n",
    "\n",
    "    1) Spectral density: Shows how the signal's energy is spread across different frequencies\n",
    "    2) Peak Frequencies: Once the spectral density is mapped, peak frequency is just the frequency/frequencies at which the spectral density is the highest, i.e the dominant frequency/frequencies.\n",
    "\n",
    "d) Decomposition-based features: Breaking down the time series into trend, seasonal and residual components, we get the following features:\n",
    "\n",
    "    1) Trend: The overall direction in which the data is moving over time, up OR down OR stationary.\n",
    "    2) Seasonal: A pattern in the time series data that keeps repeating at regular intervals.\n",
    "    3) Residual(Noisy): The random variations or sudden spikes in data after removing trend and seasonal components from the time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(df):\n",
    "    features = {}\n",
    "    for i, col in enumerate(df.columns[1:-1]):\n",
    "        data = df[col].astype(float)\n",
    "        features[f\"min{i+1}\"] = data.min()\n",
    "        features[f\"max{i+1}\"] = data.max()\n",
    "        features[f\"mean{i+1}\"] = data.mean()\n",
    "        features[f\"median{i+1}\"] = data.median()\n",
    "        features[f\"std{i+1}\"] = data.std()\n",
    "        features[f\"1st quart{i+1}\"] = data.quantile(0.25)\n",
    "        features[f\"3rd quart{i+1}\"] = data.quantile(0.75)\n",
    "    return features\n",
    "\n",
    "train_features = []\n",
    "test_features = []\n",
    "for activity, train_dataframes in train_data.items():\n",
    "    for df in train_dataframes:\n",
    "        training_data_features = extraction(df)\n",
    "        train_features.append(training_data_features)\n",
    "\n",
    "training_features = pd.DataFrame(train_features)\n",
    "\n",
    "for activity, test_dataframes in test_data.items():\n",
    "    for df in test_dataframes:\n",
    "        test_data_features = extraction(df)\n",
    "        test_features.append(test_data_features)\n",
    "\n",
    "testing_features = pd.DataFrame(test_features)\n",
    "\n",
    "training_features.insert(0, \"Instance\", range(1,len(training_features)+1))\n",
    "testing_features.insert(0, \"Instance\", range(1,len(testing_features)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the table as shown in the question, I have merged both the training and testing features dataframes to get the final new dataset dataframe which has 88 rows that depict each dataset instance from every class and 43 columns (Instance , (6 features * 7 classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.concat([training_features, testing_features], ignore_index = True)\n",
    "new_dataset['Instance'] = range(1, len(new_dataset) + 1)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstr_cols = new_dataset.drop(\"Instance\", axis=1)\n",
    "bootstr_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_samples = 1000\n",
    "bootstr_sample_metrics = {}\n",
    "\n",
    "for col in bootstr_cols.columns:\n",
    "    bootstr_samples_std = np.zeros(no_of_samples)\n",
    "    sample_data = pd.to_numeric(bootstr_cols[col]).dropna().values\n",
    "\n",
    "    for i in range(no_of_samples):\n",
    "        b_samp = np.random.choice(sample_data, size = len(sample_data), replace = True)\n",
    "        b_samp_std = np.std(b_samp, ddof=1)\n",
    "        bootstr_samples_std[i] = b_samp_std\n",
    "\n",
    "    \n",
    "    estimated_btstp_std = np.mean(bootstr_samples_std)\n",
    "    estimated_confidence_interval = np.percentile(bootstr_samples_std, [5.0,95.0])\n",
    "\n",
    "    bootstr_sample_metrics[col] = {\n",
    "        'estimated standard deviation':estimated_btstp_std,\n",
    "        'estimated 90% confidence interval':estimated_confidence_interval,\n",
    "    }\n",
    "\n",
    "bootstr_sample_metrics_df = pd.DataFrame(bootstr_sample_metrics).T\n",
    "bootstr_sample_metrics_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to me, from the time-domain features I extracted in c(ii), the min, median and max are the three most important time features.\n",
    "    \n",
    "    a) Min: The min helps us understand what is the lowest value in the time series data. In the case of the AREM dataset, it could signify when the person is at rest as the sensors will all output a low value since there is no movement.\n",
    "    \n",
    "    b) Median: The median gives the central value after sorting which helps us understand the 50-50 distribution of the range of data above and below this value. It is a sort of average value but not exactly. I chose median over mean because in the case of real world data such as sensor data where outliers can heavily exist due to machine malfunction or other problems, the mean will keep changing and shifting, not giving a nearly accurate value above or below which could exist the remaining 50% of value range. The median however doesnt change much even if outlier data could exist as it will always output the central value after sorting and this can give us a clear picture of which value can be understood as the threshold.\n",
    "\n",
    "    c) Max: The max helps us understand which is the highest value in the time series data. In the case of the AREM dataset, it can signify when the user is at the peak of his activity as the sensors will output a high value based on intensity of the activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For true linear relationship between X and Y, the linear model can very well fit the features during training as there is no complex non linear patterns in the data to be mapped. So it will have less training RSS as well. However, a cubic model can also fit the random noise in the data if it exists due to its aibility to capture more deeper non-linear patterns which the linear model can't do so easily, thus leading to the cubic model quite possibly overfitting the data in this case. \n",
    "\n",
    "And so a cubic model, for a true linear relationship of data, during training, will mostly have a smaller or in some few cases, a similar RSS than a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a true linear relationship between X and Y, during testing, a linear model could be able to generalize a little better than a cubic model as a linear model would not have overfitted the data if there was random noise, and wouldve captured the linear relationship in the data quite well on its own, thus leading it to work really well on the test data too. A cubic model would also work well on the test data but there is a good chance it could overfit the training data, and learn random noise that isnt relevant to prediction, and generalize more poorly than the linear model on the test data. \n",
    "\n",
    "Thus the cubic model will mostly have a larger or in some cases, a similar test RSS than the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a non linear relationship between X and Y, a cubic model will most definitely outperform a linear model during training due to its ability to capture non-linear patterns due to its greater number of polynomial parameters that can do so. The cubic model would have the best training RSS if the true relationship was cubic, but whatever the degree of non-linearity be, the cubic model would better fit the training data patterns (including random noise which could again arise the problem of overfitting) than a linear model.\n",
    "\n",
    "Thus, for this case, the Cubic model will have a smaller training RSS than a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a non-linear relationship between X and Y, during testing, a cubic model will again outperform the linear model, due to its ability to understand non-linear relationships and patterns in the data due to its polynomial parameters , much better than a linear model, and so it will generalize much better to non-linear data compared to a linear model. \n",
    "\n",
    "So, in this case as well, the cubic model will have a smaller training RSS than the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.3 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the model will be:\n",
    "\n",
    "SS =  50 + 20(GPA) + 0.07(IQ) + 35(LEVEL) + 0.01(GPA*IQ) - 10(GPA*LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "\n",
    "SScol = 50 + 20(GPA) + 0.07(IQ) + 35(1) + 0.01(GPA*IQ) - 10(GPA*1)        #Level = 1 for college student\n",
    "\n",
    "SShs = 50 + 20(GPA) + 0.07(IQ) + 35(0) + 0.01(GPA*IQ) - 10(GPA*0)         #Level = 0 for high school student\n",
    "\n",
    "DIFFSS = SScol - SShs\n",
    "       = 35 - 10(GPA)\n",
    "\n",
    "    i) GPA > 3.5 :- DIFFSS = -ve :- High School student earns more\n",
    "    ii) GPA < 3.5 :- DIFFSS = +ve :- College Student earns more\n",
    "\n",
    "So the correct option is (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "SScol = 50 + 20*(4.0) + 0.07*(110) + 35*(1) + 0.01*(4.0*110) - 10*(4.0*1)\n",
    "print(f\"The starting salary of a college graduate with IQ of 110 and a GPA of 4.0 is {SScol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) False, the interaction term doesnt just depend on the coefficient value, information such as the variation in the interaction term and factors such as standard errors, p values around the estimate also hugely contribute to an interaction terms relevance/power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.5 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "\n",
    "a) https://www.askpython.com/python/examples/bootstrap-sampling-introduction\n",
    "\n",
    "b) https://www.geeksforgeeks.org/data-analysis/feature-engineering-for-time-series-data-methods-and-applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
